# ğŸš€ Sistema ETL de Recursos Humanos - DataTech Solutions

> **Proyecto Formativo de Data Engineering:** Proceso ETL completo con dashboard web para gestiÃ³n de datos de RRHH

![Python](https://img.shields.io/badge/Python-3.10-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104-green.svg)
![Docker](https://img.shields.io/badge/Docker-Compose-blue.svg)
![Kafka](https://img.shields.io/badge/Apache-Kafka-red.svg)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-blue.svg)
![MongoDB](https://img.shields.io/badge/MongoDB-7.0-green.svg)
![Redis](https://img.shields.io/badge/Redis-Latest-red.svg)

## ğŸ“‹ DescripciÃ³n del Proyecto

Este es un **sistema ETL (Extract, Transform, Load) completo** diseÃ±ado para procesar datos de empleados en tiempo real. El proyecto combina mÃºltiples tecnologÃ­as modernas para crear una pipeline robusta de procesamiento de datos con interfaz web interactiva.

### ğŸ¯ Objetivos
- Procesar datos de empleados desde Kafka en tiempo real
- Almacenar datos en mÃºltiples bases de datos (PostgreSQL, MongoDB, MySQL)
- Proporcionar un dashboard web moderno para visualizaciÃ³n
- Implementar cache inteligente con Redis
- Generar analytics y reportes en tiempo real

### âœ¨ CaracterÃ­sticas Principales
- âš¡ **Procesamiento en tiempo real** con Apache Kafka
- ğŸ—„ï¸ **Multi-base de datos:** PostgreSQL, MongoDB, MySQL, Redis
- ğŸŒ **Dashboard web moderno** con FastAPI + HTML/CSS/JS
- ğŸ“Š **Analytics interactivos** con Chart.js
- ğŸ³ **ContainerizaciÃ³n completa** con Docker Compose
- ğŸ”„ **Pipeline ETL automatizada** con monitoreo
- ğŸ“± **Interfaz responsive** y moderna

## ğŸ—ï¸ Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Kafka Topic   â”‚â”€â”€â”€â–¶â”‚ ETL Consumer â”‚â”€â”€â”€â–¶â”‚   Databases     â”‚
â”‚  (HR Data)      â”‚    â”‚  (Python)    â”‚    â”‚ (Multi-DB)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                      â”‚
                              â–¼                      â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  FastAPI Web    â”‚â—€â”€â”€â”€â”‚  Redis Cache    â”‚
                    â”‚   Dashboard     â”‚    â”‚   (Temporal)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”„ Flujo de Datos
1. **Extract:** Kafka consume mensajes con datos de empleados
2. **Transform:** ETL Consumer procesa y valida los datos
3. **Load:** Almacenamiento en PostgreSQL (principal) + MongoDB (raw) + MySQL (normalizado)
4. **Cache:** Redis almacena datos temporales para optimizaciÃ³n
5. **Visualize:** Dashboard web muestra mÃ©tricas en tiempo real

## ğŸ“ Estructura Detallada del Proyecto

```
f5-de1-etl-hr/
â”œâ”€â”€ ğŸ“„ README.md                     # Esta documentaciÃ³n
â”œâ”€â”€ ğŸ“„ requirements.txt              # Dependencias Python
â”œâ”€â”€ ğŸ“„ pyproject.toml               # ConfiguraciÃ³n del proyecto
â”œâ”€â”€ ğŸ“„ docker-compose.yml           # OrquestaciÃ³n de servicios
â”œâ”€â”€ ğŸ“„ Dockerfile                   # Imagen del ETL consumer
â”œâ”€â”€ ğŸ“„ entrypoint.sh                # Script de inicio del container
â”œâ”€â”€ ğŸ“„ run_fastapi.py               # Launcher para el dashboard
â”œâ”€â”€ ğŸ“„ uv.lock                      # Lock file de dependencias
â”‚
â”œâ”€â”€ ğŸ“ config/                      # âš™ï¸ Configuraciones
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ logger_config.py         # ConfiguraciÃ³n de logging
â”‚   â”œâ”€â”€ ğŸ“„ logger_test.py           # Tests de logging
â”‚   â””â”€â”€ ğŸ“„ mongodb_config.py        # Config especÃ­fica de MongoDB
â”‚
â”œâ”€â”€ ğŸ“ fastapi/                     # ğŸŒ AplicaciÃ³n Web (Dashboard)
â”‚   â”œâ”€â”€ ğŸ“„ main.py                  # App principal FastAPI
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ routers/                 # ğŸ›£ï¸ Rutas de la API
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ api.py               # Endpoints de datos y stats
â”‚   â”‚   â””â”€â”€ ğŸ“„ frontend.py          # Rutas del frontend web
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ templates/               # ğŸ“„ Templates HTML
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ index.html           # Dashboard principal
â”‚   â”‚   â””â”€â”€ ğŸ“„ analytics.html       # PÃ¡gina de analytics avanzados
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ static/                  # ğŸ¨ Assets estÃ¡ticos
â”‚       â”œâ”€â”€ ğŸ“ css/
â”‚       â”‚   â””â”€â”€ ğŸ“„ style.css        # Estilos personalizados
â”‚       â”œâ”€â”€ ğŸ“ images/
â”‚       â”‚   â””â”€â”€ ğŸ“„ DataTech Solutions.png # Logo corporativo
â”‚       â””â”€â”€ ğŸ“ js/                  # JavaScript
â”‚           â”œâ”€â”€ ğŸ“„ app.js           # Funciones principales del dashboard
â”‚           â”œâ”€â”€ ğŸ“„ dashboard.js     # LÃ³gica especÃ­fica del dashboard
â”‚           â”œâ”€â”€ ğŸ“„ analytics.js     # Analytics page logic
â”‚           â””â”€â”€ ğŸ“„ analytics_new.js # Analytics mejorados
â”‚
â”œâ”€â”€ ğŸ“ kafka_consumer/              # âš™ï¸ Procesador ETL Principal
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ consumer.py              # Consumidor principal de Kafka
â”‚   â”œâ”€â”€ ğŸ“„ etl.py                   # LÃ³gica de transformaciÃ³n de datos
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ db_clients/              # ğŸ—„ï¸ Clientes de Bases de Datos
â”‚       â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”œâ”€â”€ ğŸ“„ mongo_init.py        # InicializaciÃ³n de MongoDB
â”‚       â”œâ”€â”€ ğŸ“„ mongo.py             # Cliente MongoDB (datos raw)
â”‚       â”œâ”€â”€ ğŸ“„ mysql_init.py        # InicializaciÃ³n y modelos MySQL
â”‚       â”œâ”€â”€ ğŸ“„ redis.py             # Cliente Redis (cache temporal)
â”‚       â””â”€â”€ ğŸ“„ sql.py               # Cliente PostgreSQL (datos finales)
â”‚
â”œâ”€â”€ ğŸ“ sql/                         # ğŸ“Š Scripts SQL
â”‚   â””â”€â”€ ğŸ“„ init.sql                 # Scripts de inicializaciÃ³n de BD
â”‚
â”œâ”€â”€ ğŸ“ notebooks/                   # ğŸ““ Jupyter Notebooks (anÃ¡lisis)
â”‚
â””â”€â”€ ğŸ“ tests/                       # ğŸ§ª Tests
    â””â”€â”€ ğŸ“„ test.py                  # Suite de pruebas
```

## ğŸ› ï¸ TecnologÃ­as Utilizadas

### Backend & ETL
- **Python 3.10+** - Lenguaje principal
- **Apache Kafka** - Streaming de datos en tiempo real
- **FastAPI** - Framework web moderno y rÃ¡pido
- **SQLAlchemy** - ORM para bases de datos relacionales
- **PyMongo** - Driver MongoDB para Python
- **Redis-py** - Cliente Redis para cache

### Bases de Datos
- **PostgreSQL** - Base de datos principal (datos procesados)
- **MongoDB** - Almacenamiento de datos raw/documentos
- **MySQL** - Base de datos normalizada (relacional)
- **Redis** - Cache en memoria para datos temporales

### Frontend & UI
- **HTML5/CSS3** - Estructura y estilos
- **JavaScript ES6+** - LÃ³gica del frontend
- **Bootstrap 5** - Framework CSS responsive
- **Chart.js** - GrÃ¡ficos interactivos
- **Font Awesome** - IconografÃ­a

### DevOps & Infraestructura
- **Docker & Docker Compose** - ContainerizaciÃ³n
- **uv** - Gestor de dependencias Python moderno
- **dotenv** - GestiÃ³n de variables de entorno

## ğŸš€ GuÃ­a de InstalaciÃ³n y EjecuciÃ³n

### Pre-requisitos
- Docker y Docker Compose instalados
- Python 3.10+ (para desarrollo local)
- Git (para clonar el repositorio)

### Paso 1: Clonar el Proyecto
```bash
git clone <repository-url>
cd f5-de1-etl-hr
```

### Paso 2: Configurar Variables de Entorno
Crear archivo `.env` en la raÃ­z del proyecto:

```bash
# Kafka Configuration
KAFKA_TOPIC=hr_data
KAFKA_GROUP_ID=hr_consumer_group
KAFKA_BROKER=kafka:9092

# MongoDB Configuration
MONGO_INITDB_ROOT_USERNAME=admin
MONGO_INITDB_ROOT_PASSWORD=password123
MONGODB_DB_NAME=hr_database
MONGO_COLLECTION=raw_employee_data
MONGO_HOST=mongo_consumer
MONGO_PORT=27017

# MySQL Configuration
MYSQL_ROOT_PASSWORD=rootpassword
MYSQL_DATABASE=hr_mysql
MYSQL_USER=hr_user
MYSQL_PASSWORD=hr_password
MYSQL_HOST=mysql_consumer

# PostgreSQL Configuration
POSTGRES_DB=hr_postgres
POSTGRES_USER=hr_user
POSTGRES_PASSWORD=hr_password
POSTGRES_HOST=postgres_consumer
POSTGRES_PORT=5432

# Redis Configuration
REDIS_HOST=redis_consumer
REDIS_PORT=6379
```

### Paso 3: Levantar el Sistema Completo

#### OpciÃ³n A: Todo con Docker (Recomendado)
```bash
# Levantar todos los servicios
docker-compose up --build -d

# Ver logs del sistema
docker-compose logs -f etl_consumer
```

#### OpciÃ³n B: Desarrollo HÃ­brido
```bash
# Solo bases de datos con Docker
docker-compose up -d mongo mysql redis

# FastAPI en local para desarrollo
cd fastapi
pip install -r ../requirements.txt
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# ETL Consumer en local
python -m kafka_consumer.consumer
```

### Paso 4: Acceder al Dashboard
- **Dashboard Principal:** http://localhost:8000
- **Analytics:** http://localhost:8000/analytics  
- **API Documentation:** http://localhost:8000/docs
- **Health Check:** http://localhost:8000/health

## ğŸ“Š Funcionalidades Detalladas

### ğŸ›ï¸ Dashboard Principal (`/`)
- **MÃ©tricas en tiempo real:** Usuarios procesados, cache, sincronizaciÃ³n
- **GrÃ¡ficos interactivos:** DistribuciÃ³n por ciudades y posiciones
- **Monitor del pipeline:** Estado de Kafka â†’ Transform â†’ Load
- **Actividad reciente:** Ãšltimos usuarios procesados
- **Estado del sistema:** Conexiones y salud de servicios

### ğŸ“ˆ Analytics Avanzados (`/analytics`)
- **KPIs del sistema:** Throughput, calidad de datos, pipelines activos
- **MÃ©tricas ETL:** Tipos de datos, timeline de procesamiento
- **AnÃ¡lisis de errores:** CategorizaciÃ³n y severidad
- **Performance:** CPU, memoria, rates de procesamiento
- **ExportaciÃ³n de datos:** CSV, JSON con filtros

### ğŸ”Œ API Endpoints Principales

#### EstadÃ­sticas y Datos
- `GET /api/stats` - EstadÃ­sticas principales del dashboard
- `GET /api/users` - Lista paginada de usuarios
- `GET /api/cache/all` - Datos en cache de Redis
- `GET /api/analytics/summary` - Resumen analytics

#### Control del Pipeline
- `GET /api/pipeline/status` - Estado actual del pipeline
- `POST /api/pipeline/start` - Iniciar pipeline
- `POST /api/pipeline/stop` - Detener pipeline  
- `POST /api/pipeline/restart` - Reiniciar pipeline

#### Monitoreo
- `GET /api/health` - Health check de servicios
- `GET /api/realtime-metrics` - MÃ©tricas en tiempo real
- `GET /api/analytics/kpis` - KPIs del sistema

## ğŸ”„ Proceso ETL Detallado

### 1. **Extract (Kafka Consumer)**
```python
# El consumer escucha el topic de Kafka
consumer = KafkaConsumer(
    'hr_data',
    bootstrap_servers='kafka:9092',
    value_deserializer=lambda x: json.loads(x.decode('utf-8'))
)
```

### 2. **Transform (ValidaciÃ³n y Procesamiento)**  
```python
# ValidaciÃ³n de tipos de datos requeridos
REQUIRED_TYPES = {
    "personal": {"email", "name", "passport", "sex", "telfnumber"},
    "location": {"address", "city", "postal_code"},
    "professional": {"company", "job", "company_email"},
    "bank": {"IBAN", "salary"},
    "net": {"IPv4"}
}
```

### 3. **Load (Multi-Database Storage)**
- **MongoDB:** Datos raw sin procesar
- **Redis:** Cache temporal para datos incompletos
- **PostgreSQL:** Datos finales procesados (JSON columns)
- **MySQL:** Datos normalizados (tablas relacionales)

### 4. **Monitor (Web Dashboard)**
- VisualizaciÃ³n en tiempo real del estado
- MÃ©tricas de performance y errores
- Control manual del pipeline

## ğŸ§ª Testing y Desarrollo

### Ejecutar Tests
```bash
# Tests bÃ¡sicos
python -m pytest tests/

# Test especÃ­fico del logger
python config/logger_test.py

# Health check manual
curl http://localhost:8000/health
```

### Desarrollo Local
```bash
# Instalar dependencias
pip install -r requirements.txt

# O usar uv (mÃ¡s rÃ¡pido)
uv pip install -r requirements.txt

# Ejecutar solo el dashboard
python run_fastapi.py

# Ejecutar solo el consumer
python -m kafka_consumer.consumer
```

## ğŸ› ï¸ Comandos Ãštiles

### Docker & Servicios
```bash
# Ver estado de containers
docker-compose ps

# Ver logs especÃ­ficos
docker-compose logs kafka_consumer
docker-compose logs fastapi

# Reiniciar servicios
docker-compose restart etl_consumer

# Limpiar todo (Â¡cuidado con los datos!)
docker-compose down -v
docker system prune -f

# Acceder a un container
docker-compose exec etl_consumer bash
docker-compose exec mongo mongosh
```

### Base de Datos
```bash
# PostgreSQL
docker-compose exec postgres psql -U hr_user -d hr_postgres

# MongoDB
docker-compose exec mongo mongosh admin -u admin -p password123

# Redis
docker-compose exec redis redis-cli
```

### Logs y Debugging
```bash
# Logs en tiempo real
docker-compose logs -f --tail=100 etl_consumer

# Verificar configuraciÃ³n
docker-compose config

# Ver recursos utilizados
docker stats
```

## ğŸš¨ SoluciÃ³n de Problemas

### Problemas Comunes

#### 1. **Error de conexiÃ³n a Kafka**
```bash
# Verificar que Kafka estÃ© corriendo
docker-compose logs kafka

# Verificar topics
docker-compose exec kafka kafka-topics.sh --list --bootstrap-server localhost:9092
```

#### 2. **Error de bases de datos**
```bash
# Verificar conexiones
curl http://localhost:8000/api/health

# Reiniciar servicios de BD
docker-compose restart mongo mysql redis postgres
```

#### 3. **Dashboard no carga**
```bash
# Verificar FastAPI
curl http://localhost:8000/ping

# Ver logs del dashboard
docker-compose logs fastapi
```

#### 4. **Problemas de permisos**
```bash
# En Linux/Mac, ajustar permisos
chmod +x entrypoint.sh
sudo chown -R $USER:$USER ./data
```

### Variables de Entorno Faltantes
Si ves errores sobre variables de entorno, asegÃºrate de que el archivo `.env` estÃ© presente y configurado correctamente.

### Performance Issues
- Verificar recursos de Docker: RAM mÃ­nimo 4GB recomendado
- Monitorear logs para identificar cuellos de botella
- Usar `docker stats` para ver uso de recursos

## ğŸ“ˆ PrÃ³ximas Mejoras

### Corto Plazo
- [ ] Implementar autenticaciÃ³n y autorizaciÃ³n
- [ ] AÃ±adir mÃ¡s visualizaciones en analytics
- [ ] Implementar alertas automÃ¡ticas
- [ ] Mejorar testing coverage

### Mediano Plazo  
- [ ] IntegraciÃ³n con Apache Airflow
- [ ] Implementar data quality checks
- [ ] AÃ±adir exportaciÃ³n de reportes
- [ ] Dashboard mÃ³vil nativo

### Largo Plazo
- [ ] Machine Learning para detecciÃ³n de anomalÃ­as
- [ ] Arquitectura de microservicios
- [ ] ImplementaciÃ³n en cloud (AWS/GCP)
- [ ] Data lineage tracking

## ğŸ‘¨â€ğŸ’» Desarrollado por

**DataTech Solutions** - Especialistas en soluciones ETL y Data Engineering

### TecnologÃ­as Core
- **Backend:** Python, FastAPI, SQLAlchemy
- **Frontend:** HTML5, CSS3, JavaScript, Bootstrap
- **Datos:** PostgreSQL, MongoDB, Redis, MySQL
- **Streaming:** Apache Kafka
- **DevOps:** Docker, Docker Compose

---

### ğŸ“ Soporte y Contacto

Para preguntas, problemas o sugerencias:
- ğŸ“§ **Email:** soporte@datatech-solutions.com
- ğŸ› **Issues:** [GitHub Issues](link-to-issues)
- ğŸ“š **Docs:** [DocumentaciÃ³n tÃ©cnica](link-to-docs)

### ğŸ”— Enlaces Ãštiles
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Apache Kafka Guide](https://kafka.apache.org/documentation/)
- [Docker Compose Reference](https://docs.docker.com/compose/)
- [Chart.js Documentation](https://www.chartjs.org/docs/)

---

**Â© 2024 DataTech Solutions - Sistema ETL para Recursos Humanos**
